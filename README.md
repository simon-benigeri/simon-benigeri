### Hello ðŸ‘‹

<!--
**simon-benigeri/simon-benigeri** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
ðŸ”­ I am a PhD student in Artificial Intelligence at Northwestern University, advised by Professor Larry Birnbaum. My primary research areas are Natural Language Processing and Conversational AI. Our goal is to build practical conversational AI systems and we hope to do this by making LLMs more reliable and robust to tasks, domains, and conversational behaviors.

<!--I also perform data science and engineering work in collaboration with cardiology and AI researchers at the Northwestern Medicine [Center for Artificial Intelligence at the Bluhm Cardiovascular Institute (BCVI)](https://ai.heart.nm.org/).-->

âš¡ Iâ€™m searching for AI/ML research internships during my PhD. If you're hiring, please reach out! 

The ideal role:
- Focuses on solving clear problems, i.e., What problem are we solving? Can you give me an example? Why is this important?
- Requires creative problem solving. I enjoy reading research papers to find new ideas to implement. 

ðŸ“« How to reach me:
- simon.benigeri@northwestern.edu
- https://www.linkedin.com/in/simon-benigeri-30993338/
- My personal [website](simonbenigeri.com)

:school: I'm always trying to learn. Here are some of my recent personal projects:
- [CalendarBot](https://github.com/simon-benigeri/calendar-bot/): A RAG chatbot for calendar event retrieval, using a hybrid approach (date-based and vector-based retrieval). It runs on Google Gemini and can be set up to work with your Google Calendar data.
- [LearningToAsk](https://github.com/c-col/LearningToAsk/): We tested LLM planning and reasoning capabilities by playing 20 questions, leveraging datasets from prior work by [Mazzaccara et. al 2024 - EMNLP](https://github.com/dmazzaccara/LearningToAsk): We developed a custom backend to simulate the game, measuring LLM performance (win percentage, turns per win, information gain per question). To enhance reasoning, we applied test-time scaling using the budget forcing technique from the paper [S1: Simple test-time scaling](https://arxiv.org/abs/2501.19393).
- [NU-NLP: Evaluating Summarizers](https://github.com/nu-nlp/evaluating-summarizers): A framework to train and evaluate neural abstractive (BART, Pegasus, T5) and optimization based summarizers (e.g. TextRank) on a variety of datasets. This code supported the experiments for the paper [Multi-domain Summarization from Leaderboards to Practice: Re-examining Automatic and Human Evaluation (Demeter et al. 2023)](https://aclanthology.org/2023.gem-1.20/).
- [Listening to elephants](https://www.fruitpunch.ai/blog/listening-to-the-giants-protecting-forest-elephants-through-audio-monitoring): Collaborated with [FruitPunch.ai](https://www.fruitpunch.ai/) to train and deploy models detecting elephant rumbles and gunshots from rainforest audio. We focused on translating a research project to the real-world, optimizing audio processing and inference, and deploying deep learning models on edge devices.
- [Dope Image Classifier](https://github.com/kobe-org/dope-image-classifier): An ML engineering project on image classification (CIFAR-10) using tools like PyTorch Lightning, Optuna, Weights & Biases, RedisAI, ONNX, and Streamlit.
- [Deep Q Trading Agent](https://github.com/lukesalamone/deep-q-trading-agent): A project using deep reinforcement learning to trade stocks from  past data. 
- [LSTM Language Model](https://github.com/simon-benigeri/lstm-language-model): Built and trained a word-level language model with LSTM on Wikitext-2 and NY Times Covid-19 articles. Explored hyperparameter effects like dropout and embedding tying.
- [Low Precision Machine Learning](https://github.com/simon-benigeri/low_precision_ml): Experimented with quantization error and stochastic rounding using low-precision floating-point representations.

<!--
- [CalendarBot](https://github.com/simon-benigeri/calendar-bot/). I built a basic Retrieval-Augmented Generation (RAG) chatbot for calendar event retrieval using a hybrid approach: date-based retrieval followed by vector-based retrieval. The space of intents is pretty limited but I'm happy with how we implemented date extractions from user queries because the most obvious parameter for relevant event retrieval is the date. It runs on Google Gemini and you can test it on your own google calendar data if you follow the set up instructions.
- [LearningToAsk](https://github.com/c-col/LearningToAsk/). My colleagues and I probed LLM planning and reasoning capabilities by testing them on the game 20 questions. We leverage datasets from prior work by [Mazzaccara et. al 2024 - EMNLP](https://github.com/dmazzaccara/LearningToAsk). But we built our own backend to simulate the game, to evaluate LLM reasoning (i.e., win percentage, turns per win, information gain per question) on the game. We also implemented test-time scaling using the budget forcing technique from the paper [S1: Simple test-time scaling](https://arxiv.org/abs/2501.19393) to improve reasoning performance.
- [Dope image classifier](https://github.com/kobe-org/dope-image-classifier). My friend `mkobbi` and I chose a simple project, like image classification on CIFAR10, and we focus on the ML engineering aspects. It's a good way to to get experience with technologies like Pytorch-lightning, optuna, weights and biases, RedisAI, ONNX, and streamlit.
- [Improving financial trading decisions with deep RL and transfer learning](https://github.com/lukesalamone/deep-q-trading-agent) is a project where my colleagues and I implement a Deep Q learning agent to trade stocks. We "made profit" on past data but don't use this agent for your own investments. It tanked on Covid market data because the action space is limited to BUY, HOLD, SELL. 
- [LSTM language model](https://github.com/simon-benigeri/lstm-language-model) is a project where my colleagues and I implemented and trained a word-level language model with LSTM. We train on 2 corpora: Wikitext-2 and NY Times articles on covid-19. Our learning goals were to create a pretraining dataset, understand the LSTM architecture, measure the effects of different hyperparameters and architectural decisions (e.g. dropout, tied embeddings).
- [Low Precision Machine Learning](https://github.com/simon-benigeri/low_precision_ml). I set up a code base to run experiments that measure quantization error, i.e., error due to training ML algorithms in low precision floating point representations. The code also simululates stochastic rounding to see if it helps with quantization error. The current repo uses a very simple model and toy datasets so we don't notice any error due to quantization. However, you can replace the model, the dataset, and run your own experiment (e.g., with a CNN we noticed clearer differences on CIFA10. 
-->
